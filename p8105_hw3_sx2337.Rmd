---
title: "p8105_hw3_sx2337"
author: "Shun Xie"
date: "2022-10-06"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
Include all the packages and load options so that tibble data will only print first five rows:
```{r}
library('tidyverse')
options(tibble.print_min = 5)
```
# Problem 1
```{r}
library(p8105.datasets)
data("instacart")
instacart
```
There are ```r nrow(instacart)``` number of samples measuring the number of orders and product being purchased in each cart. Each product has its aisle and department 

Q1:
```{r}
instacart %>% 
  distinct(aisle) %>% 
  nrow()
```

```{r}
instacart %>% 
  group_by(aisle) %>% 
  distinct() %>% 
  summarize(num=n()) %>% 
  arrange(desc(num))
```
Fresh vegetables has the most item order.

Q2
```{r}
instacart %>% 
  group_by(aisle) %>% 
  distinct() %>% 
  mutate(num=n()) %>% 
  ungroup %>% 
  filter(num>10000) %>% 
  mutate(aisle = fct_reorder(aisle,num)) %>% 
  ggplot(aes(x = aisle,y=num))+geom_bar(stat="identity")+coord_flip()+labs(
    title = "number of order in each aisle")

```

Q3
```{r}
instacart %>% 
  filter(aisle=="baking ingredients"|
           aisle=="dog food care" |
           aisle=="packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(num=n()) %>% 
  ungroup() %>% 
  arrange(desc(num)) %>% 
  group_by(aisle) %>% 
  slice(1:3) 
```
Q4:
```{r}
instacart %>% 
  filter(product_name=="Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```




# Problem 2

I am given the five weeks of accelerometer data collected on a 63 year-old male with BMI 25. The data is loaded and cleaned name here:
```{r}
acc_data = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names()
acc_data
```

The data has week, day_id, day and 1440 activity with their respective measured number. The data is too long to be read and therefore, I will do the following procedure:

1. privot long and create a column that store the activtity number and corresponding activity count. 
2. create a column specifying weekday or weekend.
3. encode data with reasonable variable classes. Additionally, I factor the day variable so that day is leveled from Monday to Sunday in order.
4. rearrange the order of columns and make week, day etc in the front and activity_minute and activity_count at the end. 

```{r}
acc_data_tidy = acc_data %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = 'activity_minute',
    names_prefix = "activity_",
    values_to = 'activity_count'
  ) %>% 
  mutate(
    weekday_or_weekend = ifelse(day=='Saturday' | day=='Sunday', "weekend", "weekday")
    ) %>% 
  mutate(
    week = as.integer(week),
    day_id = as.integer(day_id),
    day = factor(day,levels= c("Monday", 
    "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday","Sunday")),
    weekday_or_weekend = factor(weekday_or_weekend)
) %>% 
  select(week:day,weekday_or_weekend,everything())
acc_data_tidy
```
The resulting data now has 6 variables, namely the week variable specifying the week number, day_id variable specifying the id of the day, day variable which measures the day is Monday to Sunday, activity_minute variable measures for each minute of a 24-hour day starting at midnight, activity_count measures the count of activity during the one minute interval and weekday_or_weekend variable specifying the day is weekday or weekend. 



In the second part, I create a table that summarize the total activity count of each day by following:

1. using group_by to select the distint pairs of week and day variable
2. summarize the sum respect to week and day
3. create the table using pivot_wider

```{r}
acc_data_tidy %>% 
  group_by(week,day) %>% 
  summarize(total_count = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_count
  )
```
There is no apparent obvious trend according to the table. It seems like on last two weeks of Saturday, the old male does not do many activity and activity count during weekdays are relatively stable. 



In the last part, I will make ggplot on time courses for each day. 


```{r}
acc_data_tidy %>% 
  ggplot(aes(x=activity_minute,y=activity_count, color=day)) + geom_point() +geom_line()+theme(legend.position = "bottom")+
  labs(
    title = "Activity plot",
    x = "minute of the day",
    y = "activity count")+
  scale_x_discrete(
    breaks = seq(from = 0, to = 1440, by = 120), 
    labels = seq(from = 0, to = 1440, by = 120))

```

The plotting as above. There is some up and down but on Sunday, the old man is more active when it is close to 660 (around 11am) while during week days, the old man is very active during the end of the day 1200 (around 8pm). Over all days, can see that the old man is inactive between 1440 (12pm) and 540 (9am) and he is likely to be sleeping during the time. He is more active during 1200 (around 8pm) and 660 (around 11am) according to the plot. 


# Problem 3

First, load the data:
```{r}
library(p8105.datasets)
data("ny_noaa")
ny_noaa
```

The data has ```r nrow(ny_noaa)``` number of rows of data, ranging from 1981-01-01 to 2010-12-31. It contains 7 variables, namely weather station ID as id variable, Data of observation stored in date variable, precipitation measured in tenths of mm in prcp variable, snowfall in mm stored in snow variable, the depth of snow in mm stored in snwd variable and tmax and tmin variable measuring the maximum and minimum temperation, in tenths of degrees C. 

The structure of data is such: it contains all the information from each weather station for each day and the date ranges from $1_{st}$ of January 1981 to $31_{th}$ of December 2010. The data contains a lot of missing values. Need following code to see the percentage of missing value in the data:

```{r}

no_missing_ny_noaa = na.omit(ny_noaa)
nrow(no_missing_ny_noaa)

```
There are a total of ```r nrow(ny_noaa)-nrow(no_missing_ny_noaa)```
missing data, which is ```r nrow(no_missing_ny_noaa)/nrow(ny_noaa)*100```% of the total observations. This is almost a half of all observations. Therefore, missing data is an issue. But since the observation left is ```r nrow(no_missing_ny_noaa)```, which is huge. In consequence, remove the data will not have a big problem as there is still enough data to make a conclusion. 


First, clean the data:
1. seperate variables for year month and day
2. make year month and day an integer value
3. make month into Jan, Feb, etc
4. Precipitation and snowfall are in integer unit but temperature is in character, so need to return into integers

```{r}
ny_noaa_tidy = 
  ny_noaa %>% 
  separate(date,c('year','month','day'),sep='-') %>%
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) %>% 
  mutate(month = month.abb[month]) %>% 
  mutate(tmax = as.integer(tmax), tmin = as.integer(tmin))
ny_noaa_tidy
  
```
```{r}
#create a function to obtain mode
modefunct <- function(val){
  # get the unique value
  uniqueval <- unique(val)
  # match the number of value using tabulate and find the value with maximum count using which.max
  uniqueval[which.max(tabulate(match(val, uniqueval)))]
}
modefunct(na.omit(ny_noaa_tidy$snow))
```


The most common value is ```r modefunct(na.omit(ny_noaa_tidy$snow))```. This means that in most of the day and most of the weather station, there is no snowfall. This is because most of the place only snow in winter and some of the places even do not snow for the whole year. Therefore, 0 is the most common number for snowfall. 



In the second part, I make a two-panel box plot which can be used to show the average max temperature in January and in July in each station across years.

```{r}
ny_noaa_tidy %>% 
  filter(month=='Jan'|month=='Jul') %>% 
  ggplot(aes(x = month, y = tmax)) + geom_boxplot(aes(fill = month,alpha=.5))+stat_summary(fun = "median", color = "blue")+
  labs(title = "tmax for January and July")
```
According to the boxplot, the median for maximum temperature in January is approximately 0 in tenth degree C, whereas the median value for maximum temperature in July is approximately 270. Hence July has a median higher than the maximum temperature in January. However, can be seen in the blox plot, there are many outliers in the boxplot for January than the one in July. More importantly, the maximum temperature in January is more spread out than the one in July, as indicated by the thicker interquartile range and a greater range of values. The maximum outlier for January can be as high as 600 tenth of degree C, which is higher than the maximum temperature outlier for July, and as cold as -300 tenth of degree C, which is lower than the minimum temperature in July. 


In the last part, I will first make a boxplot for tmax vs tmin for the full dataset by first merge the tmax and tmin together into a temperature value using pivot longer and plot boxplot using ggplot:
```{r}
ny_noaa_tidy %>% 
  pivot_longer(
    c(tmin,tmax),
    names_to = "temperature_type",
    values_to = "tval"
  ) %>% 
  ggplot(aes(x=temperature_type,y=tval))+ geom_boxplot(aes(fill = temperature_type,alpha=.5))+stat_summary(fun = "median", color = "blue")+
  labs(title = "Boxplot for tmax and tmin")
  
```


Can be seen from the boxplot, tmax has a median of approximately 150 whereas tmin has a median of around 30. tmin also has a wider range than tmax.

Then I make a box plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
ny_noaa_tidy %>% 
  filter(snow>0 & snow<100) %>% 
  ggplot(aes(x=factor(year),y=snow))+ geom_boxplot(aes(fill = factor(year),alpha=.5))+stat_summary(fun = "median", color = "blue")+
  labs(title = "Boxplot for snow fall over years")+coord_flip()
```

Can be seen by the boxplots, over the 30 years the median of snowfall is persistently around 25mm. In the year from 1981 to 1997, the interquartile range are almost the same. Only when it comes to year 2006 the box plot has a lower quartile than other years with a few outliers ranging from 75mm  to 100mm. 




